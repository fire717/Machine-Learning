{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA模型应用：一眼看穿希拉里的邮件\n",
    "\n",
    "我们拿到希拉里泄露的邮件，跑一把LDA，看看她平时都在聊什么。\n",
    "\n",
    "首先，导入我们需要的一些库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，把希婆的邮件读取进来。\n",
    "\n",
    "这里我们用pandas。不熟悉pandas的朋友，可以用python标准库csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                  ExtractedBodyText\n",
      "1        2  B6\\nThursday, March 3, 2011 9:45 PM\\nH: Latest...\n",
      "2        3                                                Thx\n",
      "4        5  H <hrod17@clintonemail.com>\\nFriday, March 11,...\n",
      "5        6  Pis print.\\n-•-...-^\\nH < hrod17@clintonernail...\n",
      "7        8  H <hrod17@clintonemail.corn>\\nFriday, March 11...\n",
      "8        9                                                FYI\n",
      "9       10  B6\\nWednesday, September 12, 2012 6:16 PM\\nFwd...\n",
      "10      11                                       Fyi\\nB6\\n— —\n",
      "11      12  B6\\nWednesday, September 12, 2012 6:16 PM\\nFwd...\n",
      "12      13                                                Fyi\n",
      "13      14  Anne-Marie Slaughter\\nSunday, March 13, 2011 9...\n",
      "14      15  _ .....\\nFrom Randolph, Lawrence M\\nSent: Wedn...\n",
      "15      16  I asked to attend your svtc today with Embassy...\n",
      "16      17               Hope. See picture below Kamala sent.\n",
      "17      18                                     Another photo.\n",
      "18      19                                      This is nice.\n",
      "19      20  Amazing.\\nSullivan, Jacob J <Sullivanii@state,...\n",
      "20      21  H <hrod17@clintonernaii.com›\\nWednesday, Septe...\n",
      "21      22  Pis print.\\nH < hrod17@clintoriernail.corn>\\nW...\n",
      "22      23                                         Pis print.\n",
      "23      24  Follow Up Flag: Follow up\\nFlag Status: Flagge...\n",
      "24      25                                                 B5\n",
      "25      26  Sidney Blumenthal\\nThursday, September 13, 201...\n",
      "26      27                                           Will do.\n",
      "27      28                               Remind me to discuss\n",
      "28      29  http://religion.b1ogs.cnn.com/20 1 2/09/13/my-...\n",
      "29      30                                     See note below\n",
      "30      31                                                FYI\n",
      "31      32                                   embassy in Yemen\n",
      "32      33  B6\\nSunday, March 27, 201i 9:05 PM\\nH: Lots of...\n",
      "...    ...                                                ...\n",
      "7907  7908  He was larger than life in everything he did -...\n",
      "7908  7909                          OMG — I just opened this!\n",
      "7909  7910  Huma Abedin <Huma@clintonemail.com>\\nMonday, D...\n",
      "7910  7911  Are still here and so appreciative for all you...\n",
      "7911  7912  According to the Reid staff, they will tentati...\n",
      "7912  7913  My flight takes off at 8 so call before if you...\n",
      "7914  7915  That's right; we mean 09. We'll fix it. Thanks...\n",
      "7915  7916  sbwhoeop\\nTuesday, December 14, 2010 9:35 AM\\n...\n",
      "7916  7917  Death of Holbrookell.y,J3_4.:5 _043.) ,:u1.01 ...\n",
      "7919  7920  on: 12/13/2025\\nFrom\\nTo: Verveer, Melanne S\\n...\n",
      "7920  7921  That was what lona had when she went in to tal...\n",
      "7921  7922                                                Fyi\n",
      "7922  7923  Madame Secretary, I wanted to flag for you tha...\n",
      "7923  7924  It's at the bottom of page 1:\\n...Ambassador R...\n",
      "7924  7925  I will be at a USGLC breakfast doing qddr so w...\n",
      "7925  7926  Passed the Senate 81 to 19. START vote on moti...\n",
      "7926  7927                                                 -^\n",
      "7927  7928                       Traffic below from bottom up\n",
      "7928  7929                             fyi\\nForwarded message\n",
      "7929  7930  Agree — he did all the writing and integrating...\n",
      "7930  7931  Nice\\nForgot to tell you about our harrowing c...\n",
      "7932  7933                                      Worth a read.\n",
      "7933  7934  The unsigned editorials will appear in all of ...\n",
      "7936  7937  Mills, Cheryl D <MillsCD@state.gov>\\nThursday,...\n",
      "7937  7938  THE NBC/YORKER\\nDecember 14, 2010\\nThe Envoy\\n...\n",
      "7938  7939  Hi. Sorry I haven't had a chance to see you, b...\n",
      "7939  7940  B6\\nI assume you saw this by now -- if not, it...\n",
      "7941  7942  Big change of plans in the Senate. Senator Rei...\n",
      "7943  7944  PVerveer B6\\nFriday, December 17, 2010 12:12 A...\n",
      "7944  7945                                         See below.\n",
      "\n",
      "[6742 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"F:/data/HillaryEmails.csv\")\n",
    "# 原邮件数据中有很多Nan的值，直接扔了。\n",
    "df = df[['Id','ExtractedBodyText']].dropna()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理：\n",
    "\n",
    "上过我其他NLP课程的同学都知道，文本预处理这个东西，对NLP是很重要的。\n",
    "\n",
    "我们这里，针对邮件内容，写一组正则表达式：\n",
    "\n",
    "（不熟悉正则表达式的同学，直接百度关键词，可以看到一大张Regex规则表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_email_text(text):\n",
    "    text = text.replace('\\n',\" \") #新行，我们是不需要的\n",
    "    text = re.sub(r\"-\", \" \", text) #把 \"-\" 的两个单词，分开。（比如：july-edu ==> july edu）\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text) #日期，对主体模型没什么意义\n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text) #时间，没意义\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text) #邮件地址，没意义\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text) #网址，没意义\n",
    "    pure_text = ''\n",
    "    # 以防还有其他特殊字符（数字）等等，我们直接把他们loop一遍，过滤掉\n",
    "    for letter in text:\n",
    "        # 只留下字母和空格\n",
    "        if letter.isalpha() or letter==' ':\n",
    "            pure_text += letter\n",
    "    # 再把那些去除特殊字符后落单的单词，直接排除。\n",
    "    # 我们就只剩下有意义的单词了。\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word)>1)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的，现在我们新建一个colum，并把我们的方法跑一遍："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['ExtractedBodyText']\n",
    "docs = docs.apply(lambda s: clean_email_text(s))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好，来看看长相："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Thursday March PM Latest How Syria is aiding Qaddafi and more Sid hrc memo syria aiding libya docx hrc memo syria aiding libya docx March For Hillary'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head(1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们直接把所有的邮件内容拿出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doclist = docs.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA模型构建：\n",
    "\n",
    "好，我们用Gensim来做一次模型构建\n",
    "\n",
    "首先，我们得把我们刚刚整出来的一大波文本数据\n",
    "```\n",
    "[[一条邮件字符串]，[另一条邮件字符串], ...]\n",
    "```\n",
    "\n",
    "转化成Gensim认可的语料库形式：\n",
    "\n",
    "```\n",
    "[[一，条，邮件，在，这里],[第，二，条，邮件，在，这里],[今天，天气，肿么，样],...]\n",
    "```\n",
    "\n",
    "引入库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "#gensim 是一个通过衡量词组（或更高级结构，如整句或文档）模式来挖掘文档语义结构的工具\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了免去讲解安装NLTK等等的麻烦，我这里直接手写一下**停止词列表**：\n",
    "\n",
    "这些词在不同语境中指代意义完全不同，但是在不同主题中的出现概率是几乎一致的。所以要去除，否则对模型的准确性有影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoplist = ['very', 'ourselves', 'am', 'doesn', 'through', 'me', 'against', 'up', 'just', 'her', 'ours', \n",
    "            'couldn', 'because', 'is', 'isn', 'it', 'only', 'in', 'such', 'too', 'mustn', 'under', 'their', \n",
    "            'if', 'to', 'my', 'himself', 'after', 'why', 'while', 'can', 'each', 'itself', 'his', 'all', 'once', \n",
    "            'herself', 'more', 'our', 'they', 'hasn', 'on', 'ma', 'them', 'its', 'where', 'did', 'll', 'you', \n",
    "            'didn', 'nor', 'as', 'now', 'before', 'those', 'yours', 'from', 'who', 'was', 'm', 'been', 'will', \n",
    "            'into', 'same', 'how', 'some', 'of', 'out', 'with', 's', 'being', 't', 'mightn', 'she', 'again', 'be', \n",
    "            'by', 'shan', 'have', 'yourselves', 'needn', 'and', 'are', 'o', 'these', 'further', 'most', 'yourself', \n",
    "            'having', 'aren', 'here', 'he', 'were', 'but', 'this', 'myself', 'own', 'we', 'so', 'i', 'does', 'both', \n",
    "            'when', 'between', 'd', 'had', 'the', 'y', 'has', 'down', 'off', 'than', 'haven', 'whom', 'wouldn', \n",
    "            'should', 've', 'over', 'themselves', 'few', 'then', 'hadn', 'what', 'until', 'won', 'no', 'about', \n",
    "            'any', 'that', 'for', 'shouldn', 'don', 'do', 'there', 'doing', 'an', 'or', 'ain', 'hers', 'wasn', \n",
    "            'weren', 'above', 'a', 'at', 'your', 'theirs', 'below', 'other', 'not', 're', 'him', 'during', 'which']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工分词：\n",
    "\n",
    "这里，英文的分词，直接就是对着空白处分割就可以了。\n",
    "\n",
    "中文的分词稍微复杂点儿，具体可以百度：CoreNLP, HaNLP, 结巴分词，等等\n",
    "\n",
    "分词的意义在于，把我们的长长的字符串原文本，转化成有意义的小元素："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in doclist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候，我们的texts就是我们需要的样子了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thursday',\n",
       " 'march',\n",
       " 'pm',\n",
       " 'latest',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'qaddafi',\n",
       " 'sid',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'hrc',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'aiding',\n",
       " 'libya',\n",
       " 'docx',\n",
       " 'march',\n",
       " 'hillary']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立语料库\n",
    "\n",
    "用标记化的方法，把每个单词用一个数字index指代，并把我们的原文本变成一条长长的数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给你们看一眼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 1), (505, 1), (506, 1), (507, 1), (508, 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个列表告诉我们，第14（从0开始是第一）个邮件中，一共6个有意义的单词（经过我们的文本预处理，并去除了停止词后）\n",
    "\n",
    "其中，36号单词出现1次，505号单词出现1次，以此类推。。。\n",
    "\n",
    "接着，我们终于可以建立模型了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，第10号分类，其中最常出现的单词是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.009*\"today\" + 0.005*\"would\" + 0.004*\"work\" + 0.004*\"call\" + 0.004*\"sent\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topic(10, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我们把所有的主题打印出来看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.010*nuclear + 0.006*us + 0.005*american + 0.005*iran + 0.005*also'),\n",
       " (1,\n",
       "  '0.019*labour + 0.016*dialogue + 0.015*doc + 0.015*strategic + 0.014*press'),\n",
       " (2, '0.007*mr + 0.007*us + 0.006*would + 0.006*new + 0.004*israel'),\n",
       " (3,\n",
       "  '0.013*israel + 0.011*israeli + 0.011*settlements + 0.007*settlement + 0.006*one'),\n",
       " (4, '0.012*us + 0.007*diplomacy + 0.006*state + 0.005*know + 0.005*would'),\n",
       " (5, '0.045*call + 0.021*yes + 0.020*thx + 0.010*ops + 0.009*also'),\n",
       " (6,\n",
       "  '0.012*obama + 0.009*percent + 0.008*republican + 0.007*republicans + 0.006*president'),\n",
       " (7,\n",
       "  '0.069*pm + 0.036*office + 0.027*secretarys + 0.021*meeting + 0.020*room'),\n",
       " (8, '0.008*would + 0.006*party + 0.006*new + 0.005*said + 0.005*us'),\n",
       " (9, '0.007*us + 0.006*would + 0.005*state + 0.005*new + 0.005*netanyahu'),\n",
       " (10, '0.007*kurdistan + 0.006*email + 0.006*see + 0.005*us + 0.005*right'),\n",
       " (11,\n",
       "  '0.007*health + 0.007*haitian + 0.006*people + 0.005*would + 0.005*plan'),\n",
       " (12, '0.012*see + 0.009*like + 0.009*back + 0.008*im + 0.008*would'),\n",
       " (13, '0.009*new + 0.007*fyi + 0.006*draft + 0.006*speech + 0.005*also'),\n",
       " (14,\n",
       "  '0.006*military + 0.006*afghanistan + 0.005*security + 0.005*said + 0.005*government'),\n",
       " (15, '0.033*ok + 0.028*pls + 0.023*print + 0.014*call + 0.011*pis'),\n",
       " (16,\n",
       "  '0.015*state + 0.008*sounds + 0.007*us + 0.006*department + 0.005*sorry'),\n",
       " (17, '0.053*fyi + 0.012*richards + 0.006*us + 0.005*like + 0.004*defenses'),\n",
       " (18, '0.043*pm + 0.021*fw + 0.018*cheryl + 0.015*mills + 0.014*huma'),\n",
       " (19, '0.012*clips + 0.007*read + 0.006*tomorrow + 0.006*see + 0.005*send')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_topics=20, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来：\n",
    "\n",
    "通过\n",
    "```\n",
    "lda.get_document_topics(bow)\n",
    "```\n",
    "或者\n",
    "```\n",
    "lda.get_term_topics(word_id)\n",
    "```\n",
    "\n",
    "两个方法，我们可以把新鲜的文本/单词，分类成20个主题中的一个。\n",
    "\n",
    "*但是注意，我们这里的文本和单词，都必须得经过同样步骤的文本预处理+词袋化，也就是说，变成数字表示每个单词的形式。*\n",
    "\n",
    "### 作业：\n",
    "\n",
    "我这里有希拉里twitter上的几条(每一空行是单独的一条)：\n",
    "\n",
    "```\n",
    "To all the little girls watching...never doubt that you are valuable and powerful & deserving of every chance & opportunity in the world.\n",
    "\n",
    "I was greeted by this heartwarming display on the corner of my street today. Thank you to all of you who did this. Happy Thanksgiving. -H\n",
    "\n",
    "Hoping everyone has a safe & Happy Thanksgiving today, & quality time with family & friends. -H\n",
    "\n",
    "Scripture tells us: Let us not grow weary in doing good, for in due season, we shall reap, if we do not lose heart.\n",
    "\n",
    "Let us have faith in each other. Let us not grow weary. Let us not lose heart. For there are more seasons to come and...more work to do\n",
    "\n",
    "We have still have not shattered that highest and hardest glass ceiling. But some day, someone will\n",
    "\n",
    "To Barack and Michelle Obama, our country owes you an enormous debt of gratitude. We thank you for your graceful, determined leadership\n",
    "\n",
    "Our constitutional democracy demands our participation, not just every four years, but all the time\n",
    "\n",
    "You represent the best of America, and being your candidate has been one of the greatest honors of my life\n",
    "\n",
    "Last night I congratulated Donald Trump and offered to work with him on behalf of our country\n",
    "\n",
    "Already voted? That's great! Now help Hillary win by signing up to make calls now\n",
    "\n",
    "It's Election Day! Millions of Americans have cast their votes for Hillary—join them and confirm where you vote\n",
    "\n",
    "We don’t want to shrink the vision of this country. We want to keep expanding it\n",
    "\n",
    "We have a chance to elect a 45th president who will build on our progress, who will finish the job\n",
    "\n",
    "I love our country, and I believe in our people, and I will never, ever quit on you. No matter what\n",
    "\n",
    "```\n",
    "\n",
    "各位同学请使用训练好的LDA模型，判断每句话各自属于哪个potic\n",
    "\n",
    "么么哒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
