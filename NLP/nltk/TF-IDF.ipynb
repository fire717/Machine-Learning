{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF: Term Frequency, 衡量一个term在文档中出现得有多频繁。频率。\n",
    "\n",
    "TF(t) = (t出现在文档中的次数) / (文档中的term总数).\n",
    "\n",
    "IDF: Inverse Document Frequency, 衡量一个term有多重要。\n",
    "\n",
    "有些词出现的很多，但是明显不是很有用。比如'is'，’the‘，’and‘之类的。\n",
    "\n",
    "为了平衡，我们把罕见的词的重要性（weight）搞高，把常见词的重要性搞低。\n",
    "\n",
    "IDF(t) = log_e(文档总数 / 含有t的文档总数).\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "\n",
    ">例子：\n",
    "\n",
    ">一个文档有100个单词，其中单词baby出现了3次。\n",
    "\n",
    ">那么，TF(baby) = (3/100) = 0.03.\n",
    "\n",
    ">好，现在我们如果有10M的文档， baby出现在其中的1000个文档中。\n",
    "\n",
    ">那么，IDF(baby) = log(10,000,000 / 1,000) = 4\n",
    "\n",
    ">所以， TF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.047619047619047616\n",
      "1 21\n",
      "0.0\n",
      "0.0\n",
      "['this', 'is', 'sentence', 'one', 'two', 'that', 'three']\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "--------\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.15694461266687282\n",
      "0.15694461266687282\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.text import TextCollection\n",
    "# 先把所有文档TextCollection类中。\n",
    "# 这个类会自动帮你断句, 做统计, 做计算\n",
    "corpus = TextCollection(['this is sentence one',\n",
    "                        'this is sentence two',\n",
    "                        'this is sentence three'])\n",
    "\n",
    "# 直接就能算出tfidf\n",
    "# (term: 一句话中的某个term, text: 这句话)\n",
    "print(corpus.tf('this', 'this is sentence four'))\n",
    "text =  'this is sentence four'\n",
    "print(text.count('this'),len(text))  #很奇怪 这里计算出现次数是1 但是text的长度居然不是单词 而是字母长度\n",
    "print(corpus.idf('this'))\n",
    "print(corpus.tf_idf('this', 'this is sentence four'))\n",
    "#this 在后面那句话中的频率 * idf(this在上面corpus出现的文档数)\n",
    "# 1/4 * log( 3/2)\n",
    "\n",
    "# 同理, 怎么得到一个标准大小的vector来表示所有的句子?\n",
    "\n",
    "# 对于每个新句子\n",
    "new_sentence = 'this is sentence five'\n",
    "standard_vocab = nltk.word_tokenize('this is sentence one two that three')\n",
    "print(standard_vocab)\n",
    "# 遍历一遍所有的vocabulary中的词:\n",
    "for word in standard_vocab:\n",
    "    print(corpus.tf_idf(word, new_sentence))\n",
    "    # 我们会得到一个巨长(=所有vocab长度的)向量\n",
    "print('--------')    \n",
    "new_sentence1 = 'one two'\n",
    "# 遍历一遍所有的vocabulary中的词:\n",
    "for word in standard_vocab:\n",
    "    print(corpus.tf_idf(word, new_sentence1))\n",
    "    # 我们会得到一个巨长(=所有vocab长度的)向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#源代码\n",
    "class TextCollection(Text):\n",
    "    \"\"\"A collection of texts, which can be loaded with list of texts, or\n",
    "    with a corpus consisting of one or more texts, and which supports\n",
    "    counting, concordancing, collocation discovery, etc.  Initialize a\n",
    "    TextCollection as follows:\n",
    "\n",
    "    >>> import nltk.corpus\n",
    "    >>> from nltk.text import TextCollection\n",
    "    >>> print('hack'); from nltk.book import text1, text2, text3\n",
    "    hack...\n",
    "    >>> gutenberg = TextCollection(nltk.corpus.gutenberg)\n",
    "    >>> mytexts = TextCollection([text1, text2, text3])\n",
    "\n",
    "    Iterating over a TextCollection produces all the tokens of all the\n",
    "    texts in order.\n",
    "    \"\"\"\n",
    "    def __init__(self, source):\n",
    "        if hasattr(source, 'words'): # bridge to the text corpus reader\n",
    "            source = [source.words(f) for f in source.fileids()]\n",
    "\n",
    "        self._texts = source\n",
    "        Text.__init__(self, LazyConcatenation(source))\n",
    "        self._idf_cache = {}\n",
    "\n",
    "    def tf(self, term, text):\n",
    "        \"\"\" The frequency of the term in text. \"\"\"\n",
    "        return text.count(term) / len(text)\n",
    "\n",
    "\n",
    "    def idf(self, term):\n",
    "        \"\"\" The number of texts in the corpus divided by the\n",
    "        number of texts that the term appears in.\n",
    "        If a term does not appear in the corpus, 0.0 is returned. \"\"\"\n",
    "        # idf values are cached for performance.\n",
    "        idf = self._idf_cache.get(term)\n",
    "        if idf is None:\n",
    "            matches = len([True for text in self._texts if term in text])\n",
    "            # FIXME Should this raise some kind of error instead?\n",
    "            idf = (log(len(self._texts) / matches) if matches else 0.0)\n",
    "            self._idf_cache[term] = idf\n",
    "        return idf\n",
    "\n",
    "\n",
    "    def tf_idf(self, term, text):\n",
    "        return self.tf(term, text) * self.idf(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
